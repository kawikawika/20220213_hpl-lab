# 20220213_hpl-lab
High-Performance Linpack benchmark solves uniformly random system of linear equations to determine the system’s floating-point computational power. The results of HPL is the optimistic performance of the system, which is usually not match in real-world applications.

The linear equations are generated by HPL parameters N, NB, P, and Q.
- N is the problem size
- NB is the the block size
- P is the number of rows
- Q is the number of columns
  
<img width="468" alt="image" src="https://user-images.githubusercontent.com/11095946/153784916-f6468e90-9f78-4e48-8387-11cf9883e9f0.png">

## HPL.dat and slurm.bat
<detail>
  <summary>HPL.dat provides HPL with the parameters it should use to run. Slurm.bat is the slurm batch script that sets enviroment variables and actually runs HPL on the cluster</summary>
  
  ### HPL.Dat
  ```
    HPLinpack benchmark input file
    Innovative Computing Laboratory, University of Tennessee
    HPL.out      output file name (if any) 
    6            device out (6=stdout,7=stderr,file)
    1            # of problems sizes (N)
    82897        Ns
    1            # of NBs
    128          NBs
    0            PMAP process mapping (0=Row-,1=Column-major)
    1            # of process grids (P x Q)
    16           Ps
    16           Qs
    16.0         threshold
    1            # of panel fact
    2            PFACTs (0=left, 1=Crout, 2=Right)
    1            # of recursive stopping criterium
    4            NBMINs (>= 1)
    1            # of panels in recursion
    2            NDIVs
    1            # of recursive panel fact.
    1            RFACTs (0=left, 1=Crout, 2=Right)
    1            # of broadcast
    1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
    1            # of lookahead depth
    1            DEPTHs (>=0)
    2            SWAP (0=bin-exch,1=long,2=mix)
    64           swapping threshold
    0            L1 in (0=transposed,1=no-transposed) form
    0            U  in (0=transposed,1=no-transposed) form
    1            Equilibration (0=no,1=yes)
    8            memory alignment in double (> 0)
  ```
</detail>

The purpose of this lab is to create a script that will generate different HPL parameters. You may use any programming language and IDE to generate this script, as long as:
- You are able to make changes to the file structure
  - i.e. read and write to files and make directories
- You are able to run linux commands from the script
```python
  # python system call to create directory called runs in the test directory
  os.system('mkdir test/runs')
```
## Step 1: Determine NB
NB is the block size:
- Want NB to be large enough to give good DGEMM performance
- If NB is too large, the cache efficiency begins to drop
- Different DGEMM libraries may have different optimal NB values, but usually that value is a multiple of 8
- Hint: restrict your testing to 64 <= NB <= 320
- [ ] Create a method that determines NB using the information above.

## Step 2: Determine N

N is the matrix size (# of equations):
- Floating point work varies as N<sup>3</sup>, communication volume varies as N<sup>2</sup>, so the computation:communication ratio improves as N increases
- 2X increase in problem size → up to 8X increase in run time
- Memory usage in GiB is approximately 8*N<sup>2</sup>/10243
- Each node has 256 GiB of memory but Slurm is configured to allow jobs to use ~80% of that
- If you want to size a 2-node (nnodes = 2) job to use approximately 70% (mem_perc = .70) of memory, then N = sqrt(mem_perc * nnodes * 256 * 10243/8) = 219325
  - Does it help for N to be a multiple of NB?
- Make sure N isn’t too small, since results of parameterization experiments for small N may not be the same as those for large N
- [ ] Create a method that determines N using the information above.

## Step 3: Determine P and Q
P and Q are the process grid dimensions
- Need P * Q = # of MPI ranks
- The process grid shouldn’t be too rectangular (e.g., 1x128 and 128x1 are not likely to be good)
- Usually P <= Q with Q/P <= 4 works well, but it’s worth experimenting with other decompositions
- For HPL, it’s best to utilize all the cores, so # of MPI ranks * # of OpenMP threads should equal the total number of cores in your job
- Note that for HPL it is generally *not* beneficial to use both a core and its hyperthread partner for computation – one should be left idle


